---
title: 'Vision Transformer (ViT) - Codificador do transformador'
publishedAt: '2024-01-21'
summary: 'Criando o codificador do transformador do ViT'
tags: ["Pytorch", "ViT", "Transformer", "Vision"]
shortTitle: 'ViT'
---

## Atenção Multi-Cabeça (MSA)

Na primeira imagem, em vermelho, está o MSA, o foco desse conteúdo. Em azul o Layer Normalization (LN) e em roxo
a conexão residual.

Para simplificar o processo, o pytorch já tem uma implementação de MSA e de LN.

* **Autoatenção multicabeças (MSA)** - `torch.nn.MultiheadAttention()`.
* **Norma (LN ou LayerNorm)** - `torch.nn.LayerNorm()`.
* **Conexão residual** - adicione a entrada a saída.

### A camada LayerNorm (LN)

Na documentação oficial do pytorch, a LN é mais utilizada em RNNs. Para o nosso caso o parâmetro principal é o 
`normalized_shape` que é o número de features da entrada, no caso do ViT é o número de patches que é 768.

<Alert type="attention">
A normalização de camada ajuda o modelo a generalizar o modelo e treinar mais rápido.
</Alert>

Uma analogia que me ajuda a entender é que a normalização é como fosse as escadas, se deixar todas com a mesma altura e 
comprimento é muito mais fácil e rápido de subir, porem se foram cada degrau com um tamanho diferente, fica mais difícil 
de subir e mais lento. Assim, normalizar seria como deixar os dados com uma distribuição semelhante (médias e desvios padrão semelhante).

<Image rounded width={800} height={600} caption="Consulta, Chave, Valor" src="/blog/msa/chave.png" />

Para a entrada para o MSA, tem a consulta, chave e valor, esses três vetores são a parte fundamental do MSA. No pytorch, 
Já existe uma função chamada `torch.nn.MultiheadAttention()` que faz o MSA. Os parâmetros são:

* `embed_dim`: dimensão de incorporação, no caso do ViT é 768.
* `num_heads`: numero de cabeças (`multihead`), quanto mais cabeças, mais paralelismo.
* `dropout`: probabilidade de dropout, no caso do ViT é 0.0.
* `batch_first`: se a entrada e saída tem o batch na primeira dimensão, no caso do ViT é True.


```python
# 1. Criar uma instancia do MSA
class MultiheadSelfAttentionBlock(nn.Module):
    """Criacao do bloco multi-head self-attention ("MSA block").
    """
    # 2. Inicialize o bloco com os parametros
    def __init__(self,
                 embedding_dim:int=768, # Camada de Embedding do ViT-Base -> (D)
                 num_heads:int=12, # Numero de cabecas (h)
                 attn_dropout:float=0): # No artigo nao utiliza dropout
        super().__init__()

        # 3. Criacao do Layer Normalization (LN)
        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)

        # 4. Criacao do Multihead Attention (MSA)
        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,
                                                    num_heads=num_heads,
                                                    dropout=attn_dropout,
                                                    batch_first=True) # nossa dimensão de lote vem primeiro?

    # 5. Criacao do forward
    def forward(self, x):
        x = self.layer_norm(x)
        attn_output, _ = self.multihead_attn(query=x,# consulta embeddings
                                             key=x, # chave embeddings
                                             value=x, # valor embeddings
                                             need_weights=False) # precisamos dos pesos ou apenas das saídas da camada?
        return attn_output
```

Nesse código somente possui o MSA sem a conexão residual.

```python
# Create an instance of MSABlock
multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1
                                                             num_heads=12) # from Table 1

# Passando o patch com o positional embedding no MSA
patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)
print(f"Input shape of MSA block: {patch_and_position_embedding.shape}")
print(f"Output shape MSA block: {patched_image_through_msa_block.shape}")
```

A entrada e a saída tem que ter a mesma dimensão e formado. Ou seja, os dados não mudam a medida que passam.

### Perceptron Multicamadas (MLP)

Um MLP pode se referir tem um significado muito genérico, pois se refere a um perceptron multicamadas, sendo uma rede neural.
No caso do ViT, um MLP se refere a uma camada linear seguida por uma não linear (GELU) e outra camada linear. O pytorch já possui 
ambas as funções prontos para uso chamado `torch.nn.Linear()` e `torch.nn.GELU()`. Além disso Possui um Dropout que é uma 
regularização que ajuda a evitar overfitting.

```python

class MLPBlock(nn.Module):
    """Criacao do bloco MLP.
    """
    # 2. Inicialize o bloco com os parametros
    def __init__(self,
                 embedding_dim:int=768, # Tamanho do embedding de entrada
                 mlp_size:int=3072, # Tamanho do MLP 
                 dropout:float=0.1): # Dropout 
        super().__init__()

        # 3. (LN))
        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)

        # 4. (MLP)
        self.mlp = nn.Sequential(
            nn.Linear(in_features=embedding_dim,
                      out_features=mlp_size),
            nn.GELU(),
            nn.Dropout(p=dropout),
            nn.Linear(in_features=mlp_size, 
                      out_features=embedding_dim), 
            nn.Dropout(p=dropout) 
        )

    # 5. Criacao do forward()
    def forward(self, x):
        x = self.layer_norm(x)
        x = self.mlp(x)
        return x
```

#### Codificador do transformador

Um codificador é geralmente uma pilha de camadas que codifica uma entrada e gera uma saída numérica.
No caso do codificador, é uma séria de alternâncias entre MSA e MLP com uma conexão residual antes do MLP e antes do próximo
codificador.

Em um pseudocódigo:
```
x_input -> MSA_block -> [MSA_block_output + x_input] -> MLP_block -> [MLP_block_output + MSA_block_output + x_input] -> ...
```

<Alert type="attention">
O modelo `resnet` tem esse nome por conta das conexões residuais. A conexão residual, resumidamente, ajuda a rede a aprender mais `profundamente`.
</Alert>

```python 

class TransformerEncoderBlock(nn.Module):
    """Criacao do bloco do codificador do Transformer."""
    # 2. Inicializar os parametros
    def __init__(self,
                 embedding_dim:int=768, # tamanho do embedding
                 num_heads:int=12, # numero de cabecas
                 mlp_size:int=3072, # tamanho do MLP
                 mlp_dropout:float=0.1, # Dropout do MLP
                 attn_dropout:float=0): # Dropout do MSA
        super().__init__()

        # 3. MSA
        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,
                                                     num_heads=num_heads,
                                                     attn_dropout=attn_dropout)

        # 4. MLP
        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,
                                   mlp_size=mlp_size,
                                   dropout=mlp_dropout)

    # 5. forward() =
    def forward(self, x):

        # 6. Conexao residual para MSA
        x =  self.msa_block(x) + x

        # 7. Conexao residual para MLP
        x = self.mlp_block(x) + x

        return x
```

<Image rounded width={800} height={600} caption="Bloco para código" src="/blog/msa/code.png" />

Para ver a arquitetura toda:

```python
transformer_encoder_block = TransformerEncoderBlock()

# summary(model=transformer_encoder_block,
#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)
#         col_names=["input_size", "output_size", "num_params", "trainable"],
#         col_width=20,
#         row_settings=["var_names"])
```

O Pytorch tem toda essa função pronta, chamada `torch.nn.TransformerEncoderLayer()` que é até 2x mais rápido e menos propensa a erros.

### Juntando tudo

Agora que temos o MSA e o MLP, podemos juntar tudo para criar o ViT.

```python
# 1. Criar a classe ViT
class ViT(nn.Module):
    # 2. Inicializar os pesos
    def __init__(self,
                 img_size:int=224, # Resolucao da imagem
                 in_channels:int=3, # Numero de canais
                 patch_size:int=16, # Tamanho do patch
                 num_transformer_layers:int=12, # numeros de camadas do transformer
                 embedding_dim:int=768, # Tamanho do embedding
                 mlp_size:int=3072, # Tamanho do MLP
                 num_heads:int=12, # Numero de cabecas
                 attn_dropout:float=0, # Dropout do MSA
                 mlp_dropout:float=0.1, # Dropout do MLP
                 embedding_dropout:float=0.1, # Dropout do embedding
                 num_classes:int=1000): # Numero de classes (1000 para o ImageNet)
        super().__init__()

        # 3. Certificar que o tamanho da imagem é divisivel pelo tamanho do patch
        assert img_size % patch_size == 0, f"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}."

        # 4. Calcular o numero de patches
        self.num_patches = (img_size * img_size) // patch_size**2

        # 5. Criar o embedding de classe
        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),
                                            requires_grad=True)

        # 6. Criar o positional embedding
        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),
                                               requires_grad=True)

        # 7. Criar o dropout do embedding
        self.embedding_dropout = nn.Dropout(p=embedding_dropout)

        # 8. Criar o patch embedding
        self.patch_embedding = PatchEmbedding(in_channels=in_channels,
                                              patch_size=patch_size,
                                              embedding_dim=embedding_dim)

        # 9. Criar o codificador do transformer (TransformerEncoderBlock já está pronto no PyTorch)
        # Note: O "*" significa "todos"
        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,
                                                                            num_heads=num_heads,
                                                                            mlp_size=mlp_size,
                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])

        # 10. Criar o classificador
        self.classifier = nn.Sequential(
            nn.LayerNorm(normalized_shape=embedding_dim),
            nn.Linear(in_features=embedding_dim,
                      out_features=num_classes)
        )

    # 11. forward() 
    def forward(self, x):

        # 12. Obter o tamanho do batch
        batch_size = x.shape[0]

        # 13. Criar a incorporacao de classe 
        class_token = self.class_embedding.expand(batch_size, -1, -1) # "-1" significa inferir a dimensão

        # 14. Criar o patch embedding
        x = self.patch_embedding(x)

        # 15. Concatenar o class token e o patch embedding
        x = torch.cat((class_token, x), dim=1)

        # 16. Adicionar o positional embedding 
        x = self.position_embedding + x

        # 17. Executar o dropout do embedding
        x = self.embedding_dropout(x)

        # 18. Passar o embedding pelo codificador do transformer 
        x = self.transformer_encoder(x)

        # 19. Colocar o indice 0 (class token) no classificador
        x = self.classifier(x[:, 0]) # executa cada amostra em um lote com índice 0

        return x
```

Para testar o vit com um tensor aleatório de entrada:

```python
set_seeds()

# Criacao de uma imagem aleatoria
random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, num_channels, height, width)

# Criacao das classes
vit = ViT(num_classes=3)

# Passando a imagem pelo ViT
vit(random_image_tensor)
```

```
tensor([[-0,2377, 0,7360, 1,2137]], grad_fn=<AddmmBackward0>)
```
No caso a saída tem que ser 3 valores, um para cada classe.

*Continua ... 8.1*