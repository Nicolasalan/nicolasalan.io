{
  "title": "Aprendizagem Por Reforço: Uma introdução a DDPG",
  "summary": "Como modelo Actor-Critic funciona de uma maneira facil de entender",
  "publishedAt": "2023-08-01",
  "tags": [
    "Deep Reinforcement Learning",
    "Actor-Critic"
  ],
  "shortTitle": "DDPG",
  "body": {
    "raw": "\n## Método baseado em valor e método baseado em politica\n\nPara entender os conceitos de um agente baseado no método de valor, como o DQN (Deep Q-Network), e um agente baseado em política, \ncomo o REINFORCE, é fundamental compreender que ambos têm como objetivo utilizar o aprendizado por reforço \npara ensinar a um agente como realizar uma tarefa.\n\nUm agente baseado em valor avalia cada ação tomada em um determinado estado atribuindo um valor numérico a ela. \nEsse valor representa o quão vantajosa a ação é para alcançar o objetivo da tarefa. Se uma ação leva o agente mais \npróximo do objetivo, ela receberá uma recompensa maior. Por outro lado, se uma ação leva o agente para longe do \nobjetivo ou tem resultados indesejáveis, ela pode receber uma recompensa menor ou até mesmo uma recompensa negativa (penalização).\n\nJá um agente baseado em política depende de um “conselheiro” para orientar suas decisões. \nNesse caso, o “conselheiro” é uma pessoa experiente que já sabe como realizar a tarefa com sucesso. O agente, \npor sua vez, é como um iniciante que está aprendendo a realizar a tarefa e segue as sugestões do “conselheiro” para tomar suas ações.\n\n### Método Baseado em Política e Método Baseado em Valor\n\n> Vamos considerar uma pessoa que está tentando jogar tênis. Após uma partida, ela retorna para casa e reflete sobre o jogo,\nbuscando melhorar seus acertos e reduzir os erros cometidos durante a partida. No dia seguinte, ela leva em conta essas \nreflexões e tenta realizar as jogadas corretas. Essa abordagem é conhecida como método baseado em política. Agora, vamos imaginar o mesmo jogador entrando em uma nova partida, mas desta vez, em vez de se concentrar apenas em ações \ncorretas, ele tenta estimar o valor do jogo total a cada jogada. Quanto mais partidas ele joga, melhor ele se torna em estimar \no placar final do jogo. Essa abordagem é chamada de método baseado em valor.\n\nNa primeira analogia, a pessoa aprende a agir corretamente, buscando melhorar suas ações com base nas experiências passadas e \nnas reflexões sobre o jogo anterior. Já na segunda analogia, a pessoa tenta aprender a avaliar a qualidade de suas ações em \nrelação ao objetivo geral (placar final do jogo), utilizando a estimativa do valor do jogo em cada jogada.\n\n## Aproximação de Funções\n\nO método baseado em valor utiliza uma rede neural ou outra função de aproximação para estimar a função de valor, que \natribui uma nota para cada ação tomada em um determinado estado. Essa aproximação pode ser feita por meio de funções de \ntransferência como `Vπ(S)` (função de valor do estado), `Qπ(S, A)` (função de valor do estado-ação) e `Aπ(S, A)`\n(função de vantagem, medindo a vantagem de tomar uma ação A em relação a outras ações no estado S).\n\nGeralmente, quando um agente aprende uma função de valor, ele pode prosseguir para o aprendizado baseado em política com \nconhecimento prévio. Nesse ponto, o agente aprende a parametrizar a política e a otimizá-la diretamente. A política é \ngeralmente estocástica, denotada como `π(s|a)`, o que significa que ela introduz aleatoriedade na escolha das ações com base em probabilidades. Isso permite ao agente explorar diferentes ações e evitar ficar preso em um único comportamento. Por outro lado, uma política determinística, denotada como π(s), sempre escolhe a mesma ação para cada estado específico, sem envolver aleatoriedade na tomada de decisões.\n\n<Image rounded width={600} height={400} caption=\"Visualizations Diagram\" src=\"/blog/model/diagram.jpeg\" />\n\n### Viés e Variação\n\nO método Actor-Critic é uma abordagem que utiliza o agente baseado em valor para aprender a função de \nvalor e, em seguida, utiliza-a como linha de base para treinar o agente baseado em política.\n\nEssa técnica é adotada para resolver o problema do alto viés relacionado à simplificação ou suposições \nfeitas pelo modelo e alta variância relacionada ao agente aprender com novos dados em geral. Ao tentar \nestimar a função de valor ou política com base no retorno G (soma das recompensas obtidas pelo agente ao \nlongo de uma sequência de ações), há alto viés e variância.\n\nExistem duas formas de estimar o retorno. Uma delas é o método de Monte Carlo, onde o agente acumula a \nrecompensa ao longo de um episódio, considerando desconto ou não. Para calcular a função de valor, ele acumula \nvários episódios, em cada um deles computando a soma das recompensas, e depois calcula a média das estimativas.\n\nA outra forma é a estimativa por TD (diferença temporal), que leva em consideração tanto as recompensas \nimediatas quanto as estimativas de recompensas futuras.\n\n#### Analogia entre viés e variação\n\n> Imagine um jogador de futebol tentando acertar o gol. O viés é quando ele sempre chuta da mesma maneira, \ntornando-se previsível e fácil de defender. Já a variação é quando ele chuta de forma muito diferente a cada \ntentativa, o que torna seus chutes imprevisíveis e pouco precisos. Para ser um bom atacante, ele precisa \nencontrar um equilíbrio entre ser consistente (viés baixo) e ter chutes precisos (variação adequada), \naumentando suas chances de marcar gols com frequência. Da mesma forma, em aprendizado de máquina, encontrar \nesse equilíbrio entre viés e variação é fundamental para desenvolver modelos com bom desempenho e \ncapacidade de generalização.\n\nEstimativa de retorno TD (diferença temporal):\n\n`V(s) ← V(s) + α * [R + γ * V(s’) — V(s)]`\n​\n\nOnde:\n\n* V(s) é o valor estimado do estado atual “s” (a estimativa de quanta recompensa o \nagente espera obter a partir desse estado).\n\n* V(s’) é o valor estimado do próximo estado “s’” (a estimativa de quanta recompensa o \nagente espera obter a partir do próximo estado).\n\n* R é a recompensa imediata recebida pelo agente após tomar uma ação no estado “s”.\n\n* α (alfa) é a taxa de aprendizado, que controla o quanto a estimativa é atualizada a cada passo de aprendizado.\n\n* γ (gama) é o fator de desconto, que controla o peso das recompensas futuras em relação \nà recompensa imediata. Ele deve estar entre 0 e 1, onde 0 significa que o agente só se \nimporta com a recompensa imediata, e 1 significa que o agente considera todas as recompensas \nfuturas igualmente importantes.\n\nEstimativa de retorno Monte Carlo:\n\n`(G) = R₁ + γ * R₂ + γ² * R₃ + … + γ^(n-1) * Rₙ​`\n\nOnde:\n\n* G é o retorno total com desconto, ou seja, a soma das recompensas (R) ponderadas pelos fatores \nde desconto **γ** em cada passo do episódio.\n* **R₁, R₂, R₃, …, Rₙ** são as recompensas obtidas em cada um dos n passos do episódio.\n* **γ (gama)** é o fator de desconto, que controla o peso das recompensas futuras em relação à recompensa \nimediata. Ele deve estar entre 0 e 1, onde 0 significa que o agente só se importa com a recompensa \nimediata, e 1 significa que o agente considera todas as recompensas futuras igualmente importantes.\n\n## Estimativa de retorno​\nUma diferença entre o uso do método de Monte Carlo é que, no início, \nele retorna muitos valores aleatórios, pois em cada episódio as recompensas e ações são aleatórias. \nIsso faz com que demore mais para aprender, mas os valores obtidos são verdadeiros, pois o método de \nMonte Carlo não faz estimativas em cima de estimativas. Por outro lado, o método TD realiza estimativas \nem cima de estimativas, pois após uma etapa de estado, ação e recompensa, não há muito mais a saber. \nEsse método aprende mais rápido, mas pode demorar mais para convergir para um aprendizado verdadeiro. \nLogo, MC possui baixo viés, mas alta variância, e o TD possui alto viés, mas baixa variância.\n\nO método *Actor-Critic* utiliza o melhor dos dois mundos, empregando o método TD para estimar o retorno \ne o método MC para estimar a função de valor.\n\nAssim, utilizamos o TD como linha de base para treinar o agente, pois o principal foco é reduzir a variância, \nadicionando um pouco de viés. Claro que o método MC não possui viés, mas possui alta variância. \nNo caso do *Actor-Critic*, onde os estados são contínuos, ter uma alta variância pode causar ainda mais complexidade ao modelo, \npois os métodos baseados em política têm alta variância.\n\nDessa forma, os agentes baseados em *Actor-Critic* aprendem treinando e ajustando as probabilidades de boas e más \nações, assim como um ator sozinho, e utilizam um crítico para distinguir as boas das más ações. Esse método é \nmais estável do que os métodos baseados em valor e requer menos dados do que os baseados em políticas.\n\n## Modelo Actor-Critic​\n\n<Image rounded width={600} height={400} caption=\"Visualizations model Actor Critic\" src=\"/blog/model/model.jpeg\" />\n\nNo método Actor-Critic, são utilizadas duas redes neurais, uma para o crítico e outra para o ator. O crítico aprende a avaliar a função de valor do estado Vπ(S) utilizando o método TD (Diferença Temporal) como estimativa. Com base nas estimativas do crítico, a função de vantagem é calculada e usada para treinar o ator.\n\nA estrutra do modelo é a seguinte:\n\n* A rede do ator recebe o estado e retorna a probabilidade de cada ação **π(a|s; θπ)**.\n* A rede do critico recebe o estado e retorna a função de valor **V(s; θv)**.\n* O estado é passado para rede do ator e gera uma ação, que é passada para o ambiente.\n* Retorna o conjunto **(s, a, r, s’) -> (estado, ação, recompensa, proximo estado)**\n* Utilizando a estimativa TD que é **r + γ * V(s') - V(s)** para treinar o critico.\n* Em seguida, calcula a função de vantagem **A(s, a) = r + γ * V(s') - V(s)** e utiliza para treinar o ator.\n\n## DDPG (Deep Deterministic Policy Gradient) Paper​\nO DDPG (*Deep Deterministic Policy Gradient*) é uma extensão do DQN que permite lidar com tarefas contínuas, \npois utiliza uma rede neural para aproximar a função de valor de ação `Q(s, a)`.\n\nNo exemplo de modelo *Actor-Critic*, o ator retorna a probabilidade de cada ação, mas no DDPG, o ator retorna a \nação diretamente, ou seja, o ator escolhe diretamente a ação a ser tomada. Essa ação é calculada através de `argmax Q(s, a)`, \nou seja, selecionando a ação que maximiza a função de valor de ação `Q(s, a)`, tornando a política determinística. \nEnquanto isso, o crítico aprende a avaliar a função de valor da ação ideal usando a ação mais acreditada pelo ator, \nou seja, avalia como a ação selecionada pelo ator é vantajosa em relação às outras ações possíveis naquele estado. \nEsse cálculo é feito por `Q(s, U(s;θu); θq)`.\n\nPara treinar a rede crítica no DDPG, é utilizado o replay buffer para armazenar experiências passadas. \nO replay buffer é uma memória que permite ao algoritmo coletar e reutilizar experiências para o treinamento da rede de forma mais eficiente.\n\nAlém disso, o DDPG utiliza uma técnica conhecida como soft update para atualizar os pesos das redes alvo e regular, \ntanto para o ator quanto para o crítico. A rede alvo é atualizada com base nos pesos da rede regular, utilizando a \nseguinte fórmula: `θ_target = τ * θ_local + (1 - τ) * θ_target`. O parâmetro τ controla a taxa de atualização dos \npesos da rede alvo e é um hiperparâmetro importante para o treinamento do DDPG.",
    "code": "var Component=(()=>{var l=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var g=(o,a)=>()=>(a||o((a={exports:{}}).exports,a),a.exports),f=(o,a)=>{for(var r in a)i(o,r,{get:a[r],enumerable:!0})},d=(o,a,r,s)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let n of u(a))!v.call(o,n)&&n!==r&&i(o,n,{get:()=>a[n],enumerable:!(s=p(a,n))||s.enumerable});return o};var b=(o,a,r)=>(r=o!=null?l(h(o)):{},d(a||!o||!o.__esModule?i(r,\"default\",{value:o,enumerable:!0}):r,o)),q=o=>d(i({},\"__esModule\",{value:!0}),o);var m=g((E,t)=>{t.exports=_jsx_runtime});var A={};f(A,{default:()=>x,frontmatter:()=>z});var e=b(m()),z={title:\"Aprendizagem Por Refor\\xE7o: Uma introdu\\xE7\\xE3o a DDPG\",publishedAt:\"2023-08-01\",summary:\"Como modelo Actor-Critic funciona de uma maneira facil de entender\",tags:[\"Deep Reinforcement Learning\",\"Actor-Critic\"],shortTitle:\"DDPG\"};function c(o){let a=Object.assign({h2:\"h2\",p:\"p\",h3:\"h3\",blockquote:\"blockquote\",code:\"code\",h4:\"h4\",ul:\"ul\",li:\"li\",strong:\"strong\",em:\"em\"},o.components),{Image:r}=a;return r||D(\"Image\",!0),(0,e.jsxs)(e.Fragment,{children:[(0,e.jsx)(a.h2,{id:\"m\\xE9todo-baseado-em-valor-e-m\\xE9todo-baseado-em-politica\",children:\"M\\xE9todo baseado em valor e m\\xE9todo baseado em politica\"}),`\n`,(0,e.jsx)(a.p,{children:`Para entender os conceitos de um agente baseado no m\\xE9todo de valor, como o DQN (Deep Q-Network), e um agente baseado em pol\\xEDtica,\ncomo o REINFORCE, \\xE9 fundamental compreender que ambos t\\xEAm como objetivo utilizar o aprendizado por refor\\xE7o\npara ensinar a um agente como realizar uma tarefa.`}),`\n`,(0,e.jsx)(a.p,{children:`Um agente baseado em valor avalia cada a\\xE7\\xE3o tomada em um determinado estado atribuindo um valor num\\xE9rico a ela.\nEsse valor representa o qu\\xE3o vantajosa a a\\xE7\\xE3o \\xE9 para alcan\\xE7ar o objetivo da tarefa. Se uma a\\xE7\\xE3o leva o agente mais\npr\\xF3ximo do objetivo, ela receber\\xE1 uma recompensa maior. Por outro lado, se uma a\\xE7\\xE3o leva o agente para longe do\nobjetivo ou tem resultados indesej\\xE1veis, ela pode receber uma recompensa menor ou at\\xE9 mesmo uma recompensa negativa (penaliza\\xE7\\xE3o).`}),`\n`,(0,e.jsx)(a.p,{children:`J\\xE1 um agente baseado em pol\\xEDtica depende de um \\u201Cconselheiro\\u201D para orientar suas decis\\xF5es.\nNesse caso, o \\u201Cconselheiro\\u201D \\xE9 uma pessoa experiente que j\\xE1 sabe como realizar a tarefa com sucesso. O agente,\npor sua vez, \\xE9 como um iniciante que est\\xE1 aprendendo a realizar a tarefa e segue as sugest\\xF5es do \\u201Cconselheiro\\u201D para tomar suas a\\xE7\\xF5es.`}),`\n`,(0,e.jsx)(a.h3,{id:\"m\\xE9todo-baseado-em-pol\\xEDtica-e-m\\xE9todo-baseado-em-valor\",children:\"M\\xE9todo Baseado em Pol\\xEDtica e M\\xE9todo Baseado em Valor\"}),`\n`,(0,e.jsxs)(a.blockquote,{children:[`\n`,(0,e.jsx)(a.p,{children:`Vamos considerar uma pessoa que est\\xE1 tentando jogar t\\xEAnis. Ap\\xF3s uma partida, ela retorna para casa e reflete sobre o jogo,\nbuscando melhorar seus acertos e reduzir os erros cometidos durante a partida. No dia seguinte, ela leva em conta essas\nreflex\\xF5es e tenta realizar as jogadas corretas. Essa abordagem \\xE9 conhecida como m\\xE9todo baseado em pol\\xEDtica. Agora, vamos imaginar o mesmo jogador entrando em uma nova partida, mas desta vez, em vez de se concentrar apenas em a\\xE7\\xF5es\ncorretas, ele tenta estimar o valor do jogo total a cada jogada. Quanto mais partidas ele joga, melhor ele se torna em estimar\no placar final do jogo. Essa abordagem \\xE9 chamada de m\\xE9todo baseado em valor.`}),`\n`]}),`\n`,(0,e.jsx)(a.p,{children:`Na primeira analogia, a pessoa aprende a agir corretamente, buscando melhorar suas a\\xE7\\xF5es com base nas experi\\xEAncias passadas e\nnas reflex\\xF5es sobre o jogo anterior. J\\xE1 na segunda analogia, a pessoa tenta aprender a avaliar a qualidade de suas a\\xE7\\xF5es em\nrela\\xE7\\xE3o ao objetivo geral (placar final do jogo), utilizando a estimativa do valor do jogo em cada jogada.`}),`\n`,(0,e.jsx)(a.h2,{id:\"aproxima\\xE7\\xE3o-de-fun\\xE7\\xF5es\",children:\"Aproxima\\xE7\\xE3o de Fun\\xE7\\xF5es\"}),`\n`,(0,e.jsxs)(a.p,{children:[`O m\\xE9todo baseado em valor utiliza uma rede neural ou outra fun\\xE7\\xE3o de aproxima\\xE7\\xE3o para estimar a fun\\xE7\\xE3o de valor, que\natribui uma nota para cada a\\xE7\\xE3o tomada em um determinado estado. Essa aproxima\\xE7\\xE3o pode ser feita por meio de fun\\xE7\\xF5es de\ntransfer\\xEAncia como `,(0,e.jsx)(a.code,{children:\"V\\u03C0(S)\"}),\" (fun\\xE7\\xE3o de valor do estado), \",(0,e.jsx)(a.code,{children:\"Q\\u03C0(S, A)\"}),\" (fun\\xE7\\xE3o de valor do estado-a\\xE7\\xE3o) e \",(0,e.jsx)(a.code,{children:\"A\\u03C0(S, A)\"}),`\n(fun\\xE7\\xE3o de vantagem, medindo a vantagem de tomar uma a\\xE7\\xE3o A em rela\\xE7\\xE3o a outras a\\xE7\\xF5es no estado S).`]}),`\n`,(0,e.jsxs)(a.p,{children:[`Geralmente, quando um agente aprende uma fun\\xE7\\xE3o de valor, ele pode prosseguir para o aprendizado baseado em pol\\xEDtica com\nconhecimento pr\\xE9vio. Nesse ponto, o agente aprende a parametrizar a pol\\xEDtica e a otimiz\\xE1-la diretamente. A pol\\xEDtica \\xE9\ngeralmente estoc\\xE1stica, denotada como `,(0,e.jsx)(a.code,{children:\"\\u03C0(s|a)\"}),\", o que significa que ela introduz aleatoriedade na escolha das a\\xE7\\xF5es com base em probabilidades. Isso permite ao agente explorar diferentes a\\xE7\\xF5es e evitar ficar preso em um \\xFAnico comportamento. Por outro lado, uma pol\\xEDtica determin\\xEDstica, denotada como \\u03C0(s), sempre escolhe a mesma a\\xE7\\xE3o para cada estado espec\\xEDfico, sem envolver aleatoriedade na tomada de decis\\xF5es.\"]}),`\n`,(0,e.jsx)(r,{rounded:!0,width:600,height:400,caption:\"Visualizations Diagram\",src:\"/blog/model/diagram.jpeg\"}),`\n`,(0,e.jsx)(a.h3,{id:\"vi\\xE9s-e-varia\\xE7\\xE3o\",children:\"Vi\\xE9s e Varia\\xE7\\xE3o\"}),`\n`,(0,e.jsx)(a.p,{children:`O m\\xE9todo Actor-Critic \\xE9 uma abordagem que utiliza o agente baseado em valor para aprender a fun\\xE7\\xE3o de\nvalor e, em seguida, utiliza-a como linha de base para treinar o agente baseado em pol\\xEDtica.`}),`\n`,(0,e.jsx)(a.p,{children:`Essa t\\xE9cnica \\xE9 adotada para resolver o problema do alto vi\\xE9s relacionado \\xE0 simplifica\\xE7\\xE3o ou suposi\\xE7\\xF5es\nfeitas pelo modelo e alta vari\\xE2ncia relacionada ao agente aprender com novos dados em geral. Ao tentar\nestimar a fun\\xE7\\xE3o de valor ou pol\\xEDtica com base no retorno G (soma das recompensas obtidas pelo agente ao\nlongo de uma sequ\\xEAncia de a\\xE7\\xF5es), h\\xE1 alto vi\\xE9s e vari\\xE2ncia.`}),`\n`,(0,e.jsx)(a.p,{children:`Existem duas formas de estimar o retorno. Uma delas \\xE9 o m\\xE9todo de Monte Carlo, onde o agente acumula a\nrecompensa ao longo de um epis\\xF3dio, considerando desconto ou n\\xE3o. Para calcular a fun\\xE7\\xE3o de valor, ele acumula\nv\\xE1rios epis\\xF3dios, em cada um deles computando a soma das recompensas, e depois calcula a m\\xE9dia das estimativas.`}),`\n`,(0,e.jsx)(a.p,{children:`A outra forma \\xE9 a estimativa por TD (diferen\\xE7a temporal), que leva em considera\\xE7\\xE3o tanto as recompensas\nimediatas quanto as estimativas de recompensas futuras.`}),`\n`,(0,e.jsx)(a.h4,{id:\"analogia-entre-vi\\xE9s-e-varia\\xE7\\xE3o\",children:\"Analogia entre vi\\xE9s e varia\\xE7\\xE3o\"}),`\n`,(0,e.jsxs)(a.blockquote,{children:[`\n`,(0,e.jsx)(a.p,{children:`Imagine um jogador de futebol tentando acertar o gol. O vi\\xE9s \\xE9 quando ele sempre chuta da mesma maneira,\ntornando-se previs\\xEDvel e f\\xE1cil de defender. J\\xE1 a varia\\xE7\\xE3o \\xE9 quando ele chuta de forma muito diferente a cada\ntentativa, o que torna seus chutes imprevis\\xEDveis e pouco precisos. Para ser um bom atacante, ele precisa\nencontrar um equil\\xEDbrio entre ser consistente (vi\\xE9s baixo) e ter chutes precisos (varia\\xE7\\xE3o adequada),\naumentando suas chances de marcar gols com frequ\\xEAncia. Da mesma forma, em aprendizado de m\\xE1quina, encontrar\nesse equil\\xEDbrio entre vi\\xE9s e varia\\xE7\\xE3o \\xE9 fundamental para desenvolver modelos com bom desempenho e\ncapacidade de generaliza\\xE7\\xE3o.`}),`\n`]}),`\n`,(0,e.jsx)(a.p,{children:\"Estimativa de retorno TD (diferen\\xE7a temporal):\"}),`\n`,(0,e.jsxs)(a.p,{children:[(0,e.jsx)(a.code,{children:\"V(s) \\u2190 V(s) + \\u03B1 * [R + \\u03B3 * V(s\\u2019) \\u2014 V(s)]\"}),`\n\\u200B`]}),`\n`,(0,e.jsx)(a.p,{children:\"Onde:\"}),`\n`,(0,e.jsxs)(a.ul,{children:[`\n`,(0,e.jsxs)(a.li,{children:[`\n`,(0,e.jsx)(a.p,{children:`V(s) \\xE9 o valor estimado do estado atual \\u201Cs\\u201D (a estimativa de quanta recompensa o\nagente espera obter a partir desse estado).`}),`\n`]}),`\n`,(0,e.jsxs)(a.li,{children:[`\n`,(0,e.jsx)(a.p,{children:`V(s\\u2019) \\xE9 o valor estimado do pr\\xF3ximo estado \\u201Cs\\u2019\\u201D (a estimativa de quanta recompensa o\nagente espera obter a partir do pr\\xF3ximo estado).`}),`\n`]}),`\n`,(0,e.jsxs)(a.li,{children:[`\n`,(0,e.jsx)(a.p,{children:\"R \\xE9 a recompensa imediata recebida pelo agente ap\\xF3s tomar uma a\\xE7\\xE3o no estado \\u201Cs\\u201D.\"}),`\n`]}),`\n`,(0,e.jsxs)(a.li,{children:[`\n`,(0,e.jsx)(a.p,{children:\"\\u03B1 (alfa) \\xE9 a taxa de aprendizado, que controla o quanto a estimativa \\xE9 atualizada a cada passo de aprendizado.\"}),`\n`]}),`\n`,(0,e.jsxs)(a.li,{children:[`\n`,(0,e.jsx)(a.p,{children:`\\u03B3 (gama) \\xE9 o fator de desconto, que controla o peso das recompensas futuras em rela\\xE7\\xE3o\n\\xE0 recompensa imediata. Ele deve estar entre 0 e 1, onde 0 significa que o agente s\\xF3 se\nimporta com a recompensa imediata, e 1 significa que o agente considera todas as recompensas\nfuturas igualmente importantes.`}),`\n`]}),`\n`]}),`\n`,(0,e.jsx)(a.p,{children:\"Estimativa de retorno Monte Carlo:\"}),`\n`,(0,e.jsx)(a.p,{children:(0,e.jsx)(a.code,{children:\"(G) = R\\u2081 + \\u03B3 * R\\u2082 + \\u03B3\\xB2 * R\\u2083 + \\u2026 + \\u03B3^(n-1) * R\\u2099\\u200B\"})}),`\n`,(0,e.jsx)(a.p,{children:\"Onde:\"}),`\n`,(0,e.jsxs)(a.ul,{children:[`\n`,(0,e.jsxs)(a.li,{children:[`G \\xE9 o retorno total com desconto, ou seja, a soma das recompensas (R) ponderadas pelos fatores\nde desconto `,(0,e.jsx)(a.strong,{children:\"\\u03B3\"}),\" em cada passo do epis\\xF3dio.\"]}),`\n`,(0,e.jsxs)(a.li,{children:[(0,e.jsx)(a.strong,{children:\"R\\u2081, R\\u2082, R\\u2083, \\u2026, R\\u2099\"}),\" s\\xE3o as recompensas obtidas em cada um dos n passos do epis\\xF3dio.\"]}),`\n`,(0,e.jsxs)(a.li,{children:[(0,e.jsx)(a.strong,{children:\"\\u03B3 (gama)\"}),` \\xE9 o fator de desconto, que controla o peso das recompensas futuras em rela\\xE7\\xE3o \\xE0 recompensa\nimediata. Ele deve estar entre 0 e 1, onde 0 significa que o agente s\\xF3 se importa com a recompensa\nimediata, e 1 significa que o agente considera todas as recompensas futuras igualmente importantes.`]}),`\n`]}),`\n`,(0,e.jsx)(a.h2,{id:\"estimativa-de-retorno\",children:\"Estimativa de retorno\\u200B\"}),`\n`,(0,e.jsx)(a.p,{children:`Uma diferen\\xE7a entre o uso do m\\xE9todo de Monte Carlo \\xE9 que, no in\\xEDcio,\nele retorna muitos valores aleat\\xF3rios, pois em cada epis\\xF3dio as recompensas e a\\xE7\\xF5es s\\xE3o aleat\\xF3rias.\nIsso faz com que demore mais para aprender, mas os valores obtidos s\\xE3o verdadeiros, pois o m\\xE9todo de\nMonte Carlo n\\xE3o faz estimativas em cima de estimativas. Por outro lado, o m\\xE9todo TD realiza estimativas\nem cima de estimativas, pois ap\\xF3s uma etapa de estado, a\\xE7\\xE3o e recompensa, n\\xE3o h\\xE1 muito mais a saber.\nEsse m\\xE9todo aprende mais r\\xE1pido, mas pode demorar mais para convergir para um aprendizado verdadeiro.\nLogo, MC possui baixo vi\\xE9s, mas alta vari\\xE2ncia, e o TD possui alto vi\\xE9s, mas baixa vari\\xE2ncia.`}),`\n`,(0,e.jsxs)(a.p,{children:[\"O m\\xE9todo \",(0,e.jsx)(a.em,{children:\"Actor-Critic\"}),` utiliza o melhor dos dois mundos, empregando o m\\xE9todo TD para estimar o retorno\ne o m\\xE9todo MC para estimar a fun\\xE7\\xE3o de valor.`]}),`\n`,(0,e.jsxs)(a.p,{children:[`Assim, utilizamos o TD como linha de base para treinar o agente, pois o principal foco \\xE9 reduzir a vari\\xE2ncia,\nadicionando um pouco de vi\\xE9s. Claro que o m\\xE9todo MC n\\xE3o possui vi\\xE9s, mas possui alta vari\\xE2ncia.\nNo caso do `,(0,e.jsx)(a.em,{children:\"Actor-Critic\"}),`, onde os estados s\\xE3o cont\\xEDnuos, ter uma alta vari\\xE2ncia pode causar ainda mais complexidade ao modelo,\npois os m\\xE9todos baseados em pol\\xEDtica t\\xEAm alta vari\\xE2ncia.`]}),`\n`,(0,e.jsxs)(a.p,{children:[\"Dessa forma, os agentes baseados em \",(0,e.jsx)(a.em,{children:\"Actor-Critic\"}),` aprendem treinando e ajustando as probabilidades de boas e m\\xE1s\na\\xE7\\xF5es, assim como um ator sozinho, e utilizam um cr\\xEDtico para distinguir as boas das m\\xE1s a\\xE7\\xF5es. Esse m\\xE9todo \\xE9\nmais est\\xE1vel do que os m\\xE9todos baseados em valor e requer menos dados do que os baseados em pol\\xEDticas.`]}),`\n`,(0,e.jsx)(a.h2,{id:\"modelo-actor-critic\",children:\"Modelo Actor-Critic\\u200B\"}),`\n`,(0,e.jsx)(r,{rounded:!0,width:600,height:400,caption:\"Visualizations model Actor Critic\",src:\"/blog/model/model.jpeg\"}),`\n`,(0,e.jsx)(a.p,{children:\"No m\\xE9todo Actor-Critic, s\\xE3o utilizadas duas redes neurais, uma para o cr\\xEDtico e outra para o ator. O cr\\xEDtico aprende a avaliar a fun\\xE7\\xE3o de valor do estado V\\u03C0(S) utilizando o m\\xE9todo TD (Diferen\\xE7a Temporal) como estimativa. Com base nas estimativas do cr\\xEDtico, a fun\\xE7\\xE3o de vantagem \\xE9 calculada e usada para treinar o ator.\"}),`\n`,(0,e.jsx)(a.p,{children:\"A estrutra do modelo \\xE9 a seguinte:\"}),`\n`,(0,e.jsxs)(a.ul,{children:[`\n`,(0,e.jsxs)(a.li,{children:[\"A rede do ator recebe o estado e retorna a probabilidade de cada a\\xE7\\xE3o \",(0,e.jsx)(a.strong,{children:\"\\u03C0(a|s; \\u03B8\\u03C0)\"}),\".\"]}),`\n`,(0,e.jsxs)(a.li,{children:[\"A rede do critico recebe o estado e retorna a fun\\xE7\\xE3o de valor \",(0,e.jsx)(a.strong,{children:\"V(s; \\u03B8v)\"}),\".\"]}),`\n`,(0,e.jsx)(a.li,{children:\"O estado \\xE9 passado para rede do ator e gera uma a\\xE7\\xE3o, que \\xE9 passada para o ambiente.\"}),`\n`,(0,e.jsxs)(a.li,{children:[\"Retorna o conjunto \",(0,e.jsx)(a.strong,{children:\"(s, a, r, s\\u2019) -> (estado, a\\xE7\\xE3o, recompensa, proximo estado)\"})]}),`\n`,(0,e.jsxs)(a.li,{children:[\"Utilizando a estimativa TD que \\xE9 \",(0,e.jsx)(a.strong,{children:\"r + \\u03B3 * V(s') - V(s)\"}),\" para treinar o critico.\"]}),`\n`,(0,e.jsxs)(a.li,{children:[\"Em seguida, calcula a fun\\xE7\\xE3o de vantagem \",(0,e.jsx)(a.strong,{children:\"A(s, a) = r + \\u03B3 * V(s') - V(s)\"}),\" e utiliza para treinar o ator.\"]}),`\n`]}),`\n`,(0,e.jsx)(a.h2,{id:\"ddpg-deep-deterministic-policy-gradient-paper\",children:\"DDPG (Deep Deterministic Policy Gradient) Paper\\u200B\"}),`\n`,(0,e.jsxs)(a.p,{children:[\"O DDPG (\",(0,e.jsx)(a.em,{children:\"Deep Deterministic Policy Gradient\"}),`) \\xE9 uma extens\\xE3o do DQN que permite lidar com tarefas cont\\xEDnuas,\npois utiliza uma rede neural para aproximar a fun\\xE7\\xE3o de valor de a\\xE7\\xE3o `,(0,e.jsx)(a.code,{children:\"Q(s, a)\"}),\".\"]}),`\n`,(0,e.jsxs)(a.p,{children:[\"No exemplo de modelo \",(0,e.jsx)(a.em,{children:\"Actor-Critic\"}),`, o ator retorna a probabilidade de cada a\\xE7\\xE3o, mas no DDPG, o ator retorna a\na\\xE7\\xE3o diretamente, ou seja, o ator escolhe diretamente a a\\xE7\\xE3o a ser tomada. Essa a\\xE7\\xE3o \\xE9 calculada atrav\\xE9s de `,(0,e.jsx)(a.code,{children:\"argmax Q(s, a)\"}),`,\nou seja, selecionando a a\\xE7\\xE3o que maximiza a fun\\xE7\\xE3o de valor de a\\xE7\\xE3o `,(0,e.jsx)(a.code,{children:\"Q(s, a)\"}),`, tornando a pol\\xEDtica determin\\xEDstica.\nEnquanto isso, o cr\\xEDtico aprende a avaliar a fun\\xE7\\xE3o de valor da a\\xE7\\xE3o ideal usando a a\\xE7\\xE3o mais acreditada pelo ator,\nou seja, avalia como a a\\xE7\\xE3o selecionada pelo ator \\xE9 vantajosa em rela\\xE7\\xE3o \\xE0s outras a\\xE7\\xF5es poss\\xEDveis naquele estado.\nEsse c\\xE1lculo \\xE9 feito por `,(0,e.jsx)(a.code,{children:\"Q(s, U(s;\\u03B8u); \\u03B8q)\"}),\".\"]}),`\n`,(0,e.jsx)(a.p,{children:`Para treinar a rede cr\\xEDtica no DDPG, \\xE9 utilizado o replay buffer para armazenar experi\\xEAncias passadas.\nO replay buffer \\xE9 uma mem\\xF3ria que permite ao algoritmo coletar e reutilizar experi\\xEAncias para o treinamento da rede de forma mais eficiente.`}),`\n`,(0,e.jsxs)(a.p,{children:[`Al\\xE9m disso, o DDPG utiliza uma t\\xE9cnica conhecida como soft update para atualizar os pesos das redes alvo e regular,\ntanto para o ator quanto para o cr\\xEDtico. A rede alvo \\xE9 atualizada com base nos pesos da rede regular, utilizando a\nseguinte f\\xF3rmula: `,(0,e.jsx)(a.code,{children:\"\\u03B8_target = \\u03C4 * \\u03B8_local + (1 - \\u03C4) * \\u03B8_target\"}),`. O par\\xE2metro \\u03C4 controla a taxa de atualiza\\xE7\\xE3o dos\npesos da rede alvo e \\xE9 um hiperpar\\xE2metro importante para o treinamento do DDPG.`]})]})}function j(o={}){let{wrapper:a}=o.components||{};return a?(0,e.jsx)(a,Object.assign({},o,{children:(0,e.jsx)(c,o)})):c(o)}var x=j;function D(o,a){throw new Error(\"Expected \"+(a?\"component\":\"object\")+\" `\"+o+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return q(A);})();\n;return Component;"
  },
  "_id": "blog/model.mdx",
  "_raw": {
    "sourceFilePath": "blog/model.mdx",
    "sourceFileName": "model.mdx",
    "sourceFileDir": "blog",
    "contentType": "mdx",
    "flattenedPath": "blog/model"
  },
  "type": "Post",
  "slug": "model",
  "image": "/blog/model/image.jpeg",
  "og": "/blog/model/image.jpeg"
}